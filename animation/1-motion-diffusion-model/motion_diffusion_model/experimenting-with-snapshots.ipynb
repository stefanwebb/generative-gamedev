{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See whether I can convert a MDM snapshot to safetensors and load in my model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = \"/home/stefanwebb/code/python/motion-diffusion-model/save/my_humanml_trans_enc_512/model000600000.pt\"\n",
    "model_snapshot = torch.load(snapshot, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_process.poseEmbedding.weight',\n",
       " 'input_process.poseEmbedding.bias',\n",
       " 'sequence_pos_encoder.pe',\n",
       " 'seqTransEncoder.layers.0.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.0.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.0.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.0.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.0.linear1.weight',\n",
       " 'seqTransEncoder.layers.0.linear1.bias',\n",
       " 'seqTransEncoder.layers.0.linear2.weight',\n",
       " 'seqTransEncoder.layers.0.linear2.bias',\n",
       " 'seqTransEncoder.layers.0.norm1.weight',\n",
       " 'seqTransEncoder.layers.0.norm1.bias',\n",
       " 'seqTransEncoder.layers.0.norm2.weight',\n",
       " 'seqTransEncoder.layers.0.norm2.bias',\n",
       " 'seqTransEncoder.layers.1.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.1.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.1.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.1.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.1.linear1.weight',\n",
       " 'seqTransEncoder.layers.1.linear1.bias',\n",
       " 'seqTransEncoder.layers.1.linear2.weight',\n",
       " 'seqTransEncoder.layers.1.linear2.bias',\n",
       " 'seqTransEncoder.layers.1.norm1.weight',\n",
       " 'seqTransEncoder.layers.1.norm1.bias',\n",
       " 'seqTransEncoder.layers.1.norm2.weight',\n",
       " 'seqTransEncoder.layers.1.norm2.bias',\n",
       " 'seqTransEncoder.layers.2.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.2.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.2.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.2.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.2.linear1.weight',\n",
       " 'seqTransEncoder.layers.2.linear1.bias',\n",
       " 'seqTransEncoder.layers.2.linear2.weight',\n",
       " 'seqTransEncoder.layers.2.linear2.bias',\n",
       " 'seqTransEncoder.layers.2.norm1.weight',\n",
       " 'seqTransEncoder.layers.2.norm1.bias',\n",
       " 'seqTransEncoder.layers.2.norm2.weight',\n",
       " 'seqTransEncoder.layers.2.norm2.bias',\n",
       " 'seqTransEncoder.layers.3.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.3.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.3.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.3.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.3.linear1.weight',\n",
       " 'seqTransEncoder.layers.3.linear1.bias',\n",
       " 'seqTransEncoder.layers.3.linear2.weight',\n",
       " 'seqTransEncoder.layers.3.linear2.bias',\n",
       " 'seqTransEncoder.layers.3.norm1.weight',\n",
       " 'seqTransEncoder.layers.3.norm1.bias',\n",
       " 'seqTransEncoder.layers.3.norm2.weight',\n",
       " 'seqTransEncoder.layers.3.norm2.bias',\n",
       " 'seqTransEncoder.layers.4.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.4.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.4.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.4.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.4.linear1.weight',\n",
       " 'seqTransEncoder.layers.4.linear1.bias',\n",
       " 'seqTransEncoder.layers.4.linear2.weight',\n",
       " 'seqTransEncoder.layers.4.linear2.bias',\n",
       " 'seqTransEncoder.layers.4.norm1.weight',\n",
       " 'seqTransEncoder.layers.4.norm1.bias',\n",
       " 'seqTransEncoder.layers.4.norm2.weight',\n",
       " 'seqTransEncoder.layers.4.norm2.bias',\n",
       " 'seqTransEncoder.layers.5.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.5.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.5.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.5.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.5.linear1.weight',\n",
       " 'seqTransEncoder.layers.5.linear1.bias',\n",
       " 'seqTransEncoder.layers.5.linear2.weight',\n",
       " 'seqTransEncoder.layers.5.linear2.bias',\n",
       " 'seqTransEncoder.layers.5.norm1.weight',\n",
       " 'seqTransEncoder.layers.5.norm1.bias',\n",
       " 'seqTransEncoder.layers.5.norm2.weight',\n",
       " 'seqTransEncoder.layers.5.norm2.bias',\n",
       " 'seqTransEncoder.layers.6.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.6.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.6.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.6.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.6.linear1.weight',\n",
       " 'seqTransEncoder.layers.6.linear1.bias',\n",
       " 'seqTransEncoder.layers.6.linear2.weight',\n",
       " 'seqTransEncoder.layers.6.linear2.bias',\n",
       " 'seqTransEncoder.layers.6.norm1.weight',\n",
       " 'seqTransEncoder.layers.6.norm1.bias',\n",
       " 'seqTransEncoder.layers.6.norm2.weight',\n",
       " 'seqTransEncoder.layers.6.norm2.bias',\n",
       " 'seqTransEncoder.layers.7.self_attn.in_proj_weight',\n",
       " 'seqTransEncoder.layers.7.self_attn.in_proj_bias',\n",
       " 'seqTransEncoder.layers.7.self_attn.out_proj.weight',\n",
       " 'seqTransEncoder.layers.7.self_attn.out_proj.bias',\n",
       " 'seqTransEncoder.layers.7.linear1.weight',\n",
       " 'seqTransEncoder.layers.7.linear1.bias',\n",
       " 'seqTransEncoder.layers.7.linear2.weight',\n",
       " 'seqTransEncoder.layers.7.linear2.bias',\n",
       " 'seqTransEncoder.layers.7.norm1.weight',\n",
       " 'seqTransEncoder.layers.7.norm1.bias',\n",
       " 'seqTransEncoder.layers.7.norm2.weight',\n",
       " 'seqTransEncoder.layers.7.norm2.bias',\n",
       " 'embed_timestep.sequence_pos_encoder.pe',\n",
       " 'embed_timestep.time_embed.0.weight',\n",
       " 'embed_timestep.time_embed.0.bias',\n",
       " 'embed_timestep.time_embed.2.weight',\n",
       " 'embed_timestep.time_embed.2.bias',\n",
       " 'embed_text.weight',\n",
       " 'embed_text.bias',\n",
       " 'output_process.poseFinal.weight',\n",
       " 'output_process.poseFinal.bias']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_snapshot.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefanwebb/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model import MotionDiffusionModel\n",
    "model = MotionDiffusionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [x for x, _ in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layers.0.self_attn.in_proj_weight',\n",
       " 'encoder.layers.0.self_attn.in_proj_bias',\n",
       " 'encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'encoder.layers.0.self_attn.out_proj.bias',\n",
       " 'encoder.layers.0.linear1.weight',\n",
       " 'encoder.layers.0.linear1.bias',\n",
       " 'encoder.layers.0.linear2.weight',\n",
       " 'encoder.layers.0.linear2.bias',\n",
       " 'encoder.layers.0.norm1.weight',\n",
       " 'encoder.layers.0.norm1.bias',\n",
       " 'encoder.layers.0.norm2.weight',\n",
       " 'encoder.layers.0.norm2.bias',\n",
       " 'encoder.layers.1.self_attn.in_proj_weight',\n",
       " 'encoder.layers.1.self_attn.in_proj_bias',\n",
       " 'encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'encoder.layers.1.self_attn.out_proj.bias',\n",
       " 'encoder.layers.1.linear1.weight',\n",
       " 'encoder.layers.1.linear1.bias',\n",
       " 'encoder.layers.1.linear2.weight',\n",
       " 'encoder.layers.1.linear2.bias',\n",
       " 'encoder.layers.1.norm1.weight',\n",
       " 'encoder.layers.1.norm1.bias',\n",
       " 'encoder.layers.1.norm2.weight',\n",
       " 'encoder.layers.1.norm2.bias',\n",
       " 'encoder.layers.2.self_attn.in_proj_weight',\n",
       " 'encoder.layers.2.self_attn.in_proj_bias',\n",
       " 'encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'encoder.layers.2.self_attn.out_proj.bias',\n",
       " 'encoder.layers.2.linear1.weight',\n",
       " 'encoder.layers.2.linear1.bias',\n",
       " 'encoder.layers.2.linear2.weight',\n",
       " 'encoder.layers.2.linear2.bias',\n",
       " 'encoder.layers.2.norm1.weight',\n",
       " 'encoder.layers.2.norm1.bias',\n",
       " 'encoder.layers.2.norm2.weight',\n",
       " 'encoder.layers.2.norm2.bias',\n",
       " 'encoder.layers.3.self_attn.in_proj_weight',\n",
       " 'encoder.layers.3.self_attn.in_proj_bias',\n",
       " 'encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'encoder.layers.3.self_attn.out_proj.bias',\n",
       " 'encoder.layers.3.linear1.weight',\n",
       " 'encoder.layers.3.linear1.bias',\n",
       " 'encoder.layers.3.linear2.weight',\n",
       " 'encoder.layers.3.linear2.bias',\n",
       " 'encoder.layers.3.norm1.weight',\n",
       " 'encoder.layers.3.norm1.bias',\n",
       " 'encoder.layers.3.norm2.weight',\n",
       " 'encoder.layers.3.norm2.bias',\n",
       " 'encoder.layers.4.self_attn.in_proj_weight',\n",
       " 'encoder.layers.4.self_attn.in_proj_bias',\n",
       " 'encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'encoder.layers.4.self_attn.out_proj.bias',\n",
       " 'encoder.layers.4.linear1.weight',\n",
       " 'encoder.layers.4.linear1.bias',\n",
       " 'encoder.layers.4.linear2.weight',\n",
       " 'encoder.layers.4.linear2.bias',\n",
       " 'encoder.layers.4.norm1.weight',\n",
       " 'encoder.layers.4.norm1.bias',\n",
       " 'encoder.layers.4.norm2.weight',\n",
       " 'encoder.layers.4.norm2.bias',\n",
       " 'encoder.layers.5.self_attn.in_proj_weight',\n",
       " 'encoder.layers.5.self_attn.in_proj_bias',\n",
       " 'encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'encoder.layers.5.self_attn.out_proj.bias',\n",
       " 'encoder.layers.5.linear1.weight',\n",
       " 'encoder.layers.5.linear1.bias',\n",
       " 'encoder.layers.5.linear2.weight',\n",
       " 'encoder.layers.5.linear2.bias',\n",
       " 'encoder.layers.5.norm1.weight',\n",
       " 'encoder.layers.5.norm1.bias',\n",
       " 'encoder.layers.5.norm2.weight',\n",
       " 'encoder.layers.5.norm2.bias',\n",
       " 'encoder.layers.6.self_attn.in_proj_weight',\n",
       " 'encoder.layers.6.self_attn.in_proj_bias',\n",
       " 'encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'encoder.layers.6.self_attn.out_proj.bias',\n",
       " 'encoder.layers.6.linear1.weight',\n",
       " 'encoder.layers.6.linear1.bias',\n",
       " 'encoder.layers.6.linear2.weight',\n",
       " 'encoder.layers.6.linear2.bias',\n",
       " 'encoder.layers.6.norm1.weight',\n",
       " 'encoder.layers.6.norm1.bias',\n",
       " 'encoder.layers.6.norm2.weight',\n",
       " 'encoder.layers.6.norm2.bias',\n",
       " 'encoder.layers.7.self_attn.in_proj_weight',\n",
       " 'encoder.layers.7.self_attn.in_proj_bias',\n",
       " 'encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'encoder.layers.7.self_attn.out_proj.bias',\n",
       " 'encoder.layers.7.linear1.weight',\n",
       " 'encoder.layers.7.linear1.bias',\n",
       " 'encoder.layers.7.linear2.weight',\n",
       " 'encoder.layers.7.linear2.bias',\n",
       " 'encoder.layers.7.norm1.weight',\n",
       " 'encoder.layers.7.norm1.bias',\n",
       " 'encoder.layers.7.norm2.weight',\n",
       " 'encoder.layers.7.norm2.bias',\n",
       " 'timestep_encoder.time_embed.0.weight',\n",
       " 'timestep_encoder.time_embed.0.bias',\n",
       " 'timestep_encoder.time_embed.2.weight',\n",
       " 'timestep_encoder.time_embed.2.bias',\n",
       " 'input_proj.weight',\n",
       " 'input_proj.bias',\n",
       " 'output_proj.weight',\n",
       " 'output_proj.bias',\n",
       " 'text_proj.weight',\n",
       " 'text_proj.bias']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements = [\n",
    "    ('seqTransEncoder', 'encoder'),\n",
    "    ('embed_timestep', 'timestep_encoder'),\n",
    "    ('embed_text', 'text_proj'),\n",
    "    ('output_process.poseFinal', 'output_proj'),\n",
    "    ('sequence_pos_encoder', 'pos_encoder'),\n",
    "    ('input_process.poseEmbedding', 'input_proj'),\n",
    "]\n",
    "\n",
    "def map_param_name(param_name):\n",
    "    for old, new in replacements:\n",
    "        param_name = param_name.replace(old, new)\n",
    "    return param_name\n",
    "\n",
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_names = list(sorted([ map_param_name(x) for x in list(model_snapshot.keys()) if not x.endswith('pe')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layers.0.linear1.bias',\n",
       " 'encoder.layers.0.linear1.weight',\n",
       " 'encoder.layers.0.linear2.bias',\n",
       " 'encoder.layers.0.linear2.weight',\n",
       " 'encoder.layers.0.norm1.bias',\n",
       " 'encoder.layers.0.norm1.weight',\n",
       " 'encoder.layers.0.norm2.bias',\n",
       " 'encoder.layers.0.norm2.weight',\n",
       " 'encoder.layers.0.self_attn.in_proj_bias',\n",
       " 'encoder.layers.0.self_attn.in_proj_weight',\n",
       " 'encoder.layers.0.self_attn.out_proj.bias',\n",
       " 'encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'encoder.layers.1.linear1.bias',\n",
       " 'encoder.layers.1.linear1.weight',\n",
       " 'encoder.layers.1.linear2.bias',\n",
       " 'encoder.layers.1.linear2.weight',\n",
       " 'encoder.layers.1.norm1.bias',\n",
       " 'encoder.layers.1.norm1.weight',\n",
       " 'encoder.layers.1.norm2.bias',\n",
       " 'encoder.layers.1.norm2.weight',\n",
       " 'encoder.layers.1.self_attn.in_proj_bias',\n",
       " 'encoder.layers.1.self_attn.in_proj_weight',\n",
       " 'encoder.layers.1.self_attn.out_proj.bias',\n",
       " 'encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'encoder.layers.2.linear1.bias',\n",
       " 'encoder.layers.2.linear1.weight',\n",
       " 'encoder.layers.2.linear2.bias',\n",
       " 'encoder.layers.2.linear2.weight',\n",
       " 'encoder.layers.2.norm1.bias',\n",
       " 'encoder.layers.2.norm1.weight',\n",
       " 'encoder.layers.2.norm2.bias',\n",
       " 'encoder.layers.2.norm2.weight',\n",
       " 'encoder.layers.2.self_attn.in_proj_bias',\n",
       " 'encoder.layers.2.self_attn.in_proj_weight',\n",
       " 'encoder.layers.2.self_attn.out_proj.bias',\n",
       " 'encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'encoder.layers.3.linear1.bias',\n",
       " 'encoder.layers.3.linear1.weight',\n",
       " 'encoder.layers.3.linear2.bias',\n",
       " 'encoder.layers.3.linear2.weight',\n",
       " 'encoder.layers.3.norm1.bias',\n",
       " 'encoder.layers.3.norm1.weight',\n",
       " 'encoder.layers.3.norm2.bias',\n",
       " 'encoder.layers.3.norm2.weight',\n",
       " 'encoder.layers.3.self_attn.in_proj_bias',\n",
       " 'encoder.layers.3.self_attn.in_proj_weight',\n",
       " 'encoder.layers.3.self_attn.out_proj.bias',\n",
       " 'encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'encoder.layers.4.linear1.bias',\n",
       " 'encoder.layers.4.linear1.weight',\n",
       " 'encoder.layers.4.linear2.bias',\n",
       " 'encoder.layers.4.linear2.weight',\n",
       " 'encoder.layers.4.norm1.bias',\n",
       " 'encoder.layers.4.norm1.weight',\n",
       " 'encoder.layers.4.norm2.bias',\n",
       " 'encoder.layers.4.norm2.weight',\n",
       " 'encoder.layers.4.self_attn.in_proj_bias',\n",
       " 'encoder.layers.4.self_attn.in_proj_weight',\n",
       " 'encoder.layers.4.self_attn.out_proj.bias',\n",
       " 'encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'encoder.layers.5.linear1.bias',\n",
       " 'encoder.layers.5.linear1.weight',\n",
       " 'encoder.layers.5.linear2.bias',\n",
       " 'encoder.layers.5.linear2.weight',\n",
       " 'encoder.layers.5.norm1.bias',\n",
       " 'encoder.layers.5.norm1.weight',\n",
       " 'encoder.layers.5.norm2.bias',\n",
       " 'encoder.layers.5.norm2.weight',\n",
       " 'encoder.layers.5.self_attn.in_proj_bias',\n",
       " 'encoder.layers.5.self_attn.in_proj_weight',\n",
       " 'encoder.layers.5.self_attn.out_proj.bias',\n",
       " 'encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'encoder.layers.6.linear1.bias',\n",
       " 'encoder.layers.6.linear1.weight',\n",
       " 'encoder.layers.6.linear2.bias',\n",
       " 'encoder.layers.6.linear2.weight',\n",
       " 'encoder.layers.6.norm1.bias',\n",
       " 'encoder.layers.6.norm1.weight',\n",
       " 'encoder.layers.6.norm2.bias',\n",
       " 'encoder.layers.6.norm2.weight',\n",
       " 'encoder.layers.6.self_attn.in_proj_bias',\n",
       " 'encoder.layers.6.self_attn.in_proj_weight',\n",
       " 'encoder.layers.6.self_attn.out_proj.bias',\n",
       " 'encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'encoder.layers.7.linear1.bias',\n",
       " 'encoder.layers.7.linear1.weight',\n",
       " 'encoder.layers.7.linear2.bias',\n",
       " 'encoder.layers.7.linear2.weight',\n",
       " 'encoder.layers.7.norm1.bias',\n",
       " 'encoder.layers.7.norm1.weight',\n",
       " 'encoder.layers.7.norm2.bias',\n",
       " 'encoder.layers.7.norm2.weight',\n",
       " 'encoder.layers.7.self_attn.in_proj_bias',\n",
       " 'encoder.layers.7.self_attn.in_proj_weight',\n",
       " 'encoder.layers.7.self_attn.out_proj.bias',\n",
       " 'encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'input_proj.bias',\n",
       " 'input_proj.weight',\n",
       " 'output_proj.bias',\n",
       " 'output_proj.weight',\n",
       " 'text_proj.bias',\n",
       " 'text_proj.weight',\n",
       " 'timestep_encoder.time_embed.0.bias',\n",
       " 'timestep_encoder.time_embed.0.weight',\n",
       " 'timestep_encoder.time_embed.2.bias',\n",
       " 'timestep_encoder.time_embed.2.weight']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(sorted([x for x, _ in list(model.named_parameters())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 106)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_names), len(mapped_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(param_names, mapped_names):\n",
    "    print(x == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_state_dict = {map_param_name(k): v for k, v in model_snapshot.items() if map_param_name(k) != 'pos_encoder.pe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(mapped_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "save_file(mapped_state_dict, \"model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
